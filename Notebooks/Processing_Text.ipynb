{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFRTuEET4qnu",
        "outputId": "654117d5-0324-4637-e739-ffecc21fdd99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Upgrading dependencies\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "uynsJYzuX7F4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX1GqmspYGp5",
        "outputId": "26ecaacb-5d96-4682-b2ba-3958ace5b75c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "G4TxQOHuYTMR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKDoPuhiYl4r",
        "outputId": "b486556c-98e6-4cea-d8fc-34c763271eee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with simple text cleaning processes"
      ],
      "metadata": {
        "id": "ibH4NNSdXg_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"   This is a message to be cleaned. It might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .  \"\n",
        "print(txt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZsokuIeXfkx",
        "outputId": "be8a6d8a-dec9-48f8-bafd-f50ccce513d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   This is a message to be cleaned. It might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing the text so that it is all in lowercase."
      ],
      "metadata": {
        "id": "78sRXdCaXtUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = txt.lower()\n",
        "print(txt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNz4Tbf0XqJl",
        "outputId": "5050cfc1-b38f-4045-cb72-2b2121c6f2ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   this is a message to be cleaned. it might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing any leading and trailing whitespaces"
      ],
      "metadata": {
        "id": "84seg1d5X0Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = txt.strip()\n",
        "print(txt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsZkUIYXX0BT",
        "outputId": "abd99c57-aaf2-47aa-d267-a0297bcf221c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a message to be cleaned. it might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Regular Expressions to remove any HTML tags or Markup\n",
        "The regular expression `<.*?>` is commonly used to match HTML or XML tags. It is a great example of how greedy vs non-greedy quantifiers work in regex.\n",
        "<br />\n",
        "**Breakdown of `<.*?>`**<br />\n",
        "`<` - Matches a literal `<` character. This is the opening of a tag. <br />\n",
        "`.*?` - This is the key part. <br />\n",
        "  `.` - Matches any character except newline <br />\n",
        "  `*` - Means 'zero or more' of the preceding element <br />\n",
        "  `?` - Makes the `*` non-greedy (i.e., match as little as possible) <br />\n",
        "`>` - Matches a literal `>` character. This is the closing of a tag. <br />\n",
        "\n"
      ],
      "metadata": {
        "id": "M108D80MZI-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = re.compile('<.*?>').sub('', txt)\n",
        "print(txt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQv_AKK0ZHRI",
        "outputId": "430e846e-9248-497a-8569-5ff8a3079107"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a message to be cleaned. it might involve some things like: , ?, :, ''  adjacent spaces, and tabs     .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying re.sub() instead of re.compile()"
      ],
      "metadata": {
        "id": "74k_BinEZd1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt1 = \"this is a message to be cleaned. it might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .\"\n",
        "txt1 = re.sub(r'<.*?>', '', txt1)\n",
        "print(txt1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rggJ1Ska6Xv",
        "outputId": "5c6f281a-91f0-49b5-87ee-0ba6bc93c3cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a message to be cleaned. it might involve some things like: , ?, :, ''  adjacent spaces, and tabs     .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replacing punctuation with space.\n",
        "We should be careful with this task, as depending on the application, punctuation can actually be useful. For instance, punctuation might affect the positive or negative meaning of a sentence. <br />\n",
        "`string.punctuation` - This is a predefined string in the `string` module that contains all standard punctuation characters.\n",
        "\n",
        "```\n",
        "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "```\n",
        "<br />\n",
        "\n",
        "The `re.escape()` function escapes all characters that could be interpreted as special regex characters. So, the result will look like this, <br />\n",
        "\n",
        "```\n",
        "\\!\\\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~\n",
        "```\n",
        "This makes sure each punctuation character is treated literally in the regex. <br />\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "'[%s]' % re.escape(string.punctuation)\n",
        "```\n",
        "This creates a regex character class. <br />\n",
        "`[%s]` is a format string. This is a placeholder for a string. <br />\n",
        "The `[...]` are literal characters in the final string. <br />\n",
        "The code inserts the escaped punctuation string inside square brackets. This final string is used as a regular expression pattern to match any one of those punctuation characters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AetzVUFnbWSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', txt)\n",
        "print(txt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLgRD0H4bI_D",
        "outputId": "ffb229f3-2cf2-4c82-9030-ff0aedc877c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a message to be cleaned  it might involve some things like              adjacent spaces  and tabs      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing any extra space and tabs"
      ],
      "metadata": {
        "id": "N2AOkpFAe5l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = re.sub('\\s+', ' ', txt)\n",
        "print(txt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILHePIPDe4HM",
        "outputId": "a0626a27-773b-41c7-841c-15b9bb8d8c6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a message to be cleaned it might involve some things like adjacent spaces and tabs \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with lexicon-based text processing\n",
        "Lexicon-based text processing methods are applied after the common text-processing methods. They are used to normalize sentences in the dataset. Normalization means putting words into a similar format that will also enhance the similarities (if any) between the sentences. <br />\n",
        "We have to install some packages, <br />\n",
        "\n",
        "\n",
        "*   `punkt` - This is a pretrained sentence tokenizer for the English language.\n",
        "*   `averaged_perceptron_tagger` - This is a part-of-speech tagger\n",
        "*   `wordnet` - This is a large database of English words that can be used to find the meanings of words, synonyms, antonyms, and more.\n",
        "\n"
      ],
      "metadata": {
        "id": "Yj0tQKRzfFcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword removal\n",
        "Some words in sentences can occur very frequently, and they don't contribute too much to the overall meaning of the sentences. Typically, we use a list of such words and remove them from each sentence. For example, stopwords include a, an, the, this, that, is, it, to, and and."
      ],
      "metadata": {
        "id": "WCSEzMTZa4cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence = []\n",
        "\n",
        "# We can adjusts stopwords according to our problem\n",
        "stopwords = ['a', 'an', 'the', 'this', 'that', 'is', 'it', 'to', 'and']\n",
        "\n",
        "# Tokenizing the sentence\n",
        "words = word_tokenize(txt)\n",
        "\n",
        "for w in words:\n",
        "  if w not in stopwords:\n",
        "    filtered_sentence.append(w)\n",
        "\n",
        "text_ = \" \".join(filtered_sentence)\n"
      ],
      "metadata": {
        "id": "fcdITHo2fIOG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28KXF8F7b5I5",
        "outputId": "5ce556e3-01e5-4916-a2e6-552763d249ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message be cleaned might involve some things like adjacent spaces tabs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming words\n",
        "Stemming is a rule-based system for converting words into their root form. It removes suffixes from words. This process helps enhance similarities (if any) between sentences. For example, \"jumping\", \"jumped\" -> \"jump\" or \"cars\" -> \"car\"."
      ],
      "metadata": {
        "id": "RihUXBN6cOAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the stemmer\n",
        "sstemmer = SnowballStemmer('english')\n"
      ],
      "metadata": {
        "id": "qcY0Eaj5b7Yc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_sentence = []\n",
        "\n",
        "# Tokenizing the sentence\n",
        "words = word_tokenize(text_)\n",
        "\n",
        "for w in words:\n",
        "  stemmed_sentence.append(sstemmer.stem(w))\n",
        "\n",
        "stemmed_text = \" \".join(stemmed_sentence)\n"
      ],
      "metadata": {
        "id": "TsnEx1KMcsDO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__kUvRgLdBlS",
        "outputId": "b66855b6-8637-4968-91f7-793f3aee5903"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messag be clean might involv some thing like adjac space tab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This stemming operation is not perfect. It generated some mistakes, such as involv, messag, and adjac. Stemming is a rule-based method that sometimes mistakenly removes suffixes from words. Nevertheless, it runs quickly."
      ],
      "metadata": {
        "id": "nKIdMuWVdHNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatizing words\n",
        "Since the result of stemming was not satisfactory, we can use lemmatization instead. It usually requires more work, but it gives better results."
      ],
      "metadata": {
        "id": "HFe4QOuMddWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the lemmatizer\n",
        "word_lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "ZoFN2AHjdFJ2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a helper function to map NTLK position tags\n",
        "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "  if tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN\n"
      ],
      "metadata": {
        "id": "wiF_BhNBd28k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_sentence = []\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = word_tokenize(text_)\n"
      ],
      "metadata": {
        "id": "T2uYtO5reUYW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z76lcjOkm0YG",
        "outputId": "a6d88eef-105f-4c3a-bb0e-1397bb8345aa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['message',\n",
              " 'be',\n",
              " 'cleaned',\n",
              " 'might',\n",
              " 'involve',\n",
              " 'some',\n",
              " 'things',\n",
              " 'like',\n",
              " 'adjacent',\n",
              " 'spaces',\n",
              " 'tabs']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting position tags\n",
        "word_pos_tags = nltk.pos_tag(words)\n"
      ],
      "metadata": {
        "id": "WNvEqRT2m1og"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of tuples\n",
        "word_pos_tags\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UMyXtMgm3r2",
        "outputId": "9c914782-9537-40bc-d362-5cf541bc5f21"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('message', 'NN'),\n",
              " ('be', 'VB'),\n",
              " ('cleaned', 'VBN'),\n",
              " ('might', 'MD'),\n",
              " ('involve', 'VB'),\n",
              " ('some', 'DT'),\n",
              " ('things', 'NNS'),\n",
              " ('like', 'IN'),\n",
              " ('adjacent', 'JJ'),\n",
              " ('spaces', 'NNS'),\n",
              " ('tabs', 'VBP')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping the position tag and lemmatizing the word or token\n",
        "for idx, tag in enumerate(word_pos_tags):\n",
        "  lemmatized_sentence.append(word_lemmatizer.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
        "\n",
        "lemmatized_text = \" \".join(lemmatized_sentence)\n"
      ],
      "metadata": {
        "id": "PG_nVp6-m5Up"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code iterates through a list of word-POS tag pairs. For each word:\n",
        "\n",
        "*   It converts the POS tag to a WordNet-compatible format,\n",
        "*   It uses a lemmatizer to find the base form (lemma) of the word, using the POS tag to improve accuracy,\n",
        "*   It adds the lemmatized word to a list called lemmatized_sentence.\n",
        "\n",
        "After the loop finishes, lemmatized_sentence will contain a list of the lemmatized words from the original sentence.\n"
      ],
      "metadata": {
        "id": "piOc5u6foXHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrmvxTqNnWbD",
        "outputId": "78bdaaa7-f0b6-40d8-f835-fa2c5732992f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message be clean might involve some thing like adjacent space tabs\n"
          ]
        }
      ]
    }
  ]
}